### Notes

#### Accuracy

*ML Fundamentals*
Accuracy is the number of correct classification predictions divided by the total number of predictions.
That is: $Accuracy = \frac{correct\ predictions}{correct\ predictions + incorrect\ predictions}$

Example: a model that made 40 correct predictions and 10 incorrect predictions would have an accuracy of:

$Accuracy = \frac{40}{40+10} = 80\%$
#### Activation function

*ML Fundamentals*
A function that enables **neural networks** to learn **nonlinear** (complex) relationships between features and the label.

Popular activation functions include:

- **ReLU**
- **Sigmoid**

The plots of a activation functions are never single straight lines. For example, the plot of the ReLU activation function consists of two straight lines.

![[ReLU.png]]

A plot of the sigmoid activation function looks as follows:

![[Pasted image 20231106164105.png]]

In a neural network, activation functions manipulate the weighted sum of all the inputs to a neuron. To calculate a weighted sum, the neuron adds up the products of the relevant values and weights. For example, suppose the relevant input to a neuron consists of the following:

| input value | input weight |
| ----------- | ------------ |
| 2           | -1.3         |
| -1          | 0.6          |
| 3           | 0.4          | 


$weighted\ sum = (2)(-1.3) + (-1)(0.6) + (3)(0.4) = -2.0$

Suppose the designer of this neural network chooses the sigmoid function to be the activation function. In that case, the neuron calculates the sigmoid of -2.0, which is approximately 0.12. Therefore, the neuron passes 0.12 (rather -2.0) to the next layer in the neural network. The following figure illustrates the relevant part of the process:

![[Pasted image 20231106164820.png]]
#### Autoencoder 

*Language evaluation*, *Image models*
A system that learns to extract the most important information from the input. Autoencoders are a combination of an **encoder** and **decoder**. Autoencoders rely on the following two-step process:

1. The encoder maps the input to a (typically) lossy lower-dimensional (intermediate) format.
2. The decoder builds a lossy version of the original input by mapping the lower-dimensional format to the original higher-dimensional input format.

Autoencoders are trained end-to-end by having the decoder attempt to reconstruct the original input from the encoder's intermediate format as closely as possible. Because the intermediate format is smaller (lower-dimensional) than the original format, the autoencoder is forced to learn what information in the input is essential, and the output won't be perfectly identical to the input.

For example:

- If the input data is a graphic, the non-exact copy would be similar to the original graphic, but somewhat modified. Perhaps the non-exact copy removes noise from the original graphic or fills in some missing pixels.
- If the input data is text, an autoencoder would generate new text that mimics (but is not identical to) the original text.
#### Back-propagation

*ML Fundamentals*

The algorithm that implements **gradient descent** in **neural networks**.

Training a neural network involves many **iterations** of the following two-pass cycle:

1. During the **forward pass**, the system processes a **batch** of **examples** to yield prediction(s). The system compares each prediction to each **label** value. The difference between the prediction and the label value is the **loss** for that example. The system aggregates the losses for all the examples to compute the total loss for the current batch.
2. During the **backward pass** (back-propagation), the system reduces loss by adjusting the weights of all the **neurons** in all the **hidden layer(s)**.

Neural networks often contain many neurons across many hidden layers. Each of those neurons contribute to the overall loss in different ways. Back-propagation determines whether to increase or decrease the weights applied to particular neurons.

The **learning rate** is a multiplier that controls the degree to which each backward pass increases or decreases each weight. A large learning rate will increase or decrease each weight more than a small learning rate.

In calculus terms, back-propagation implements calculus' **chain rule**. That is, back-propagation calculates the **partial derivative** of the error with respect to each parameter.
#### Bagging

*Decision forests*

A method to **train** an **ensemble** where each constituent **model** trains on a random subset of training examples **sampled with replacement**. For example, a **random forest** is a collection of **decision trees** trained with bagging.

The term bagging is short for bootstrap aggregating.

#### Basis function

*ML Fundamentals*
Basis functions allow modeling non linearity in the data while keeping linearity in parameters, which greatly simplifies the analysis of these models. Using linear combination of different basis functions, we can construct complex functions and still use linear regression.

Example basis functions:

- **Polynomial Basis function**
- **Gaussian Basis function**
- **Sigmoidal Basis function**

Advantages:
- Closed form solution for least-squares problem
- Tractable Bayesian treatment
- Nonlinear models mapping input variables to target variables through basis functions

Limitations:
- Assumption: Basis function $\phi_j(x)$ are fixed, not learned.
- **Curse of dimensionality**: to cover growing dimensions D of input vectors, the number of basis functions needs to grow rapidly / exponentially

#### Bias (ethics/fairness)

*Fairness*, *ML Fundamentals*

1. Stereotyping, predjudic or favoritism towards some things, people, or groups over others. These biases can affect collection and interpretation of data, the design of a system, and how users interact with a system. Forms of this bias include:
	- **automation bias**
	- **confirmation bias**
	- **experimenter's bias**
	- **group attribution bias**
	- **implicit bias**
	- **in-group bias**
	- **out-group homogeneity bias**
 
2.  Systematic error introduced by a sampling or reporting procedure. Forms of this type of bias include:
	- **coverage bias**
	- **non-response bias**
	- **participation bias**
	- **reporting bias**
	- **sampling bias**
	- **selection bias**

Not to be confused with the **bias term** in machine learning models or **prediction bias**

#### Bias (math) or bias term

*ML Fundamentals*
An intercept or offset from an origin. Bias is a parameter in machine learning models, which is symbolized by either of the following:
- b
- $w_0$

For example, bias is the b in the following formula:
$$y' = b + w_1x_1 + w_2x_2 + \dots w_nx_n$$
In a simple two-dimensional line, bias just means "y-intercept." For example, the bias of the line in the following illustration is 2.

![[Pasted image 20231106172736.png]]

Bias exists because not all models start from the origin (0,0). For example, suppose an amusement park costs 2 Euros to enter and an additional 0.5 Euro for every hour a customer stays. Therefore, a model mapping the total cost has a bias of 2 because the lowest cost is 2 Euros.

Bias is not to be confused with **bias in ethics and fairness** or **prediction bias**

#### Prediction bias

*ML Fundamentals*

A value indicating how far apart the average of **predictions** is from the average of **labels** in the dataset.

Not to be confused with the **bias term** in machine learning models or with **bias in ethics and fairness**
#### Bias-variance tradeoff

*ML Fundamentals*

Describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model.

The conflict is in trying to simultaneously minimize both bias and variance in order to generalize.

- High bias, low variance -> underfitting
- Low bias, high variance -> overfitting

![[Pasted image 20231106180907.png]]

#### Boosting

A machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as "weak" classifiers) into a classifier with high accuracy (a "strong" classifier) by **upweighting** the examples that the model is currently misclassifying.

#### Bootstrap sample (sampling with replacement)

*Decision forest*

A method of picking items from a set of candidate items in which the same item can be picked multiple times. The phrase "with replacement" means that after each selection, the selected item is returned to the pool of candidate items. The inverse method, **sampling without replacement**, means that a candidate item can only be picked once.

For example, consider the following fruit set:

fruit = {kiwi, apple, pear, fig, cherry, lime, mango}

Suppose that the system randomly picks fig as the first item. If using sampling with replacement, then the system picks the second item from the following set:

fruit = {kiwi, apple, pear, fig, cherry, lime, mango}

#### Classification

*ML Fundamentals*

Give a prediction on a given set of classes. 

There is two types of classification:

- **binary classification**
- **multi-class classification**
#### Clustering

*Clustering*

Grouping related **examples**, particularly during **unsupervised learning**. Once all the examples are grouped, a human can optionally supply meaning to each cluster.

Many clustering algorithms exist. For example, the **k-means** algorithms clusters examples based on their proximity to a **centroid**, as in the following diagram:

![[Pasted image 20231106182307.png]]

A human researcher could then review the clusters and, for example, label cluster 1 as "dwarf trees" and cluster 2 as "full-size trees".

As another example, consider a clustering algorithm based on an example's distance from a center point, illustrated as follows:

![[Pasted image 20231106182430.png]]

#### Confusion matrix

An NxN table that summarizes the number of correct and incorrect predictions tha a **classification model** made. For example, consider the following confusion matrix for a **binary classification** model:

|                          | Tumor (predicted) | Non-Tumor (predicted) |
| ------------------------ | ----------------- | --------------------- |
| Tumor (ground truth)     | 18 (TP)           | 1 (FN)                | 
| Non-Tumor (ground truth) | 6 (FP)            | 452 (TN)              |

The preceding confusion matrix shows the following:
- Of the 19 predictions in which **ground truth** was Tumor, the model correctly classified 18 and incorrectly classified 1.
- Of the 458 predictions in which ground truth was Non-Tumor, the model correctly classified 452 and incorrectly classified 6.

The confusion matrix for a **multi-class classification** problem can help you identify patterns of mistakes. For example, consider the following confusion matrix for a 3-class multi-class classification model that categorizes three different iris types (Virginica, Versicolor, and Setosa). When the ground truth was Virginica, the confusion matrix shows that the model was far more likely to mistakenly predict Versicolor than Setosa:

|                           | Setosa (predicted) | Versicolor (predicted) | Virginica (predicted) |
| ------------------------- | ------------------ | ---------------------- | --------------------- |
| Setosa (ground truth)     | 88                 | 12                     | 0                     |
| Versicolor (ground truth) | 6                  | 141                    | 7                     |
| Virginica (ground truth)  | 2                  | 27                     | 109                   |

As yet another example, a confusion matrix could reveal that a model trained to recognize handwritten digits tends to mistakenly predict 9 instead of 4, or mistakenly predict 1 instead of 7.

Confusion matrices contain sufficient information to calculate a variety of performance metrics, including **precision** and **recall**.


### Handwritten