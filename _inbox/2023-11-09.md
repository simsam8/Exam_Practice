### Notes

#### Model evaluation

Having chosen the final model, evaluating its prediction on new data.

#### Model selection (or assessment)

Estimating the performance of several models in order to choose the best one.
- Model selection is part of training

#### Naive Bayes

Naive Bayes classifiers are a collection of classification algorithms based on **Bayes’ Theorem**. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.
#### Neural Network

*ML Fundamentals*

A **model** containing at least one **hidden layer**. A **deep neural network** is a type of neural network containing more than one hidden layer. For example, the following diagram shows a deep neural network containing two hidden layers.

![[Pasted image 20231109111159.png]]

Each neuron in a neural network connects to all of the nodes in the next layer. For example, in the preceding diagram, notice that each of the three neurons in the first hidden layer separately connect to both of the two neurons in the second hidden layer.

Neural networks implemented on computers are sometimes called artificial neural networks to differentiate them from neural networks found in brains and other nervous systems.

Some neural networks can mimic extremely complex nonlinear relationships between different features and the label.

See also **convolutional neural network** and **recurrent neural network**

#### No free lunch theorem

"When averaged over all possible datasets, all classifiers perform equally well."
(Wolpert & Macready, 1997)

- Does it matter at all which algorithm we choose?
- Yes, it matters
	- The point is that if an algorithm A performs better than B on a dataset D then there must exist a dataset on which A performs worse than B
	- In real world, all possible datasets are not equally likely to occur
	- So we want to choose an algorithm that works well on datasets that are likely to occur

Implications:

- Seems to imply that there is no general-purpose algorithm
	- But allows existence of an algorithm A that performs better than B on all problems that we encounter in real life (then B must perform better than A on problems that we do not encounter in real life)
![[No free lunch.png]]
#### Overfitting

*ML Fundamentals*

Creating a **model** that matches the **training data** so closely that the model fails to make correct predictions on new data.

**Regularization** can reduce overfitting. Training on a large and diverse training set can also reduce overfitting.

Overfitting is like strictly following advice from only your favorite teacher. You'll probably be successful in that teacher's class, but you might "overfit" to that teacher's ideas and be unsuccessful in other classes. Following advice from a mixture of teachers will enable you to adapt better to new situations.

#### Oversampling

Reusing the **examples** of a **minority class** in a **class-imbalanced dataset** in order to create a more balanced **training set**.

For example, consider a **binary classification** problem in which the ratio of the **majority class** to the minority class is 5,000:1. If the dataset contains a million examples, then the dataset contains only about 200 examples of the minority class, which might be too few examples for effective training. To overcome this deficiency, you might oversample (reuse) those 200 examples multiple times, possibly yielding sufficient examples for useful training.

You need to be careful about **overfitting** when oversampling.

Contrast with **undersampling**

#### Precision

A metric for **classification model** that answers the following question:

When the model predicted the **positive** class, what percentage of the predictions were correct?

Here is the formula:

$$Precision = \frac{true\ positives}{true\ positives + false\ positives}$$
where:

- true positive means the model correctly predicted the positive class.
- false positive means the model mistakenly predicted the positive class.

For example, suppose a model made 200 positive predictions. Of these 200 positive predictions:
- 150 were true positives
- 50 were false positives

In this case:

$$Precision = \frac{150}{150+50}=0.75$$

Contrast with **accuracy** and **recall**
#### Principal component analysis (PCA)

Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.

Basic idea:
- Learn the most interesting directions (new basis functions) from the data
	- Interesting direction = lots of variation
- Represent data using these directions (change of basis)
- Throw away "uninteresting" directions without much of loss of information. This gives a lower dimensional representation.
![[PCA.png]]

Strengths:
- Works often well
- Fast and simple

Weaknesses:
- Scaling affects results
- Relies on linearity assumptions
- Maximum variance features are not necessarily the most predictive ones.

#### Pruning

Reduce the size of a decision tree by removing branches

Early stopping (pre-pruning):
	- Stop recursion once information gain is non-positive
	- Problematic, cannot handle, e.g., XOR

Pruning (post-pruning)
	- Build a full tree, remove parts of it later

#### Radial basis function

#### Random Forest

*Decision Forests*

An **ensemble** of **decision trees** in which each decision tree is trained with a specific random noise, such as **bagging**

Random forests are a type of **decision forest**.
#### Recall

A metric for **classification models** that answers the following question:

When **ground truth** was the **positive class**, what percentage of predictions did the model correctly identify as the positive class?

Here is the formula:

$$Recall = \frac{true\ positives}{true\ positives + false\ negatives}$$

where:

- true positive means the model correctly predicted the positive class.
- false negative means that the model mistakenly predicted the **negative class**

For instance, suppose your model made 200 predictions on examples for which ground truth was the positive class. Of these 200 predictions:

- 180 were true positives
- 20 were false negatives

In this case:

$$Recall = \frac{180}{180+20} = 0.9$$
Recall is particularly useful for determining the predictive power of classification models in which the positive class is rare. For example, consider a **class-imbalanced dataset** in which the positive class for a certain disease occurs in only 10 patients out of a million. Suppose your model makes five million predictions that yield the following outcomes:

- 30 True Positives
- 20 False Negatives
- 4.999.000 True Negatives
- 950 False Positives

The recall of this model is therefore:

	recall = TP / (TP + FN)
	recall = 30 / (30 + 20) = 0.6 = 60%

By contrast, the **accuracy** of this model is:

	accuracy = (TP + TN) / (TP + TN + FP + FN)
	accuarcy = (30 + 4.999.000) / (30 + 4.999.000 + 950 + 20) = 99.98%

That high value of accuracy looks impressive but is essentially meaningless.
Recall is a much more useful metric for class-imbalance datasets than accuracy.
#### Rectified Linear Unit (ReLU)

*ML Fundamentals*

An **activation function** with the following behaviour:
- If input is negative or zero, then the output is 0.
- If input is positive, then the output is equal to the input.

For example:
- If the input is -3, then the output is 0.
- If the input is +3, then the output is 3.0.

Here is a plot of ReLU:
![[Pasted image 20231109133811.png]]

ReLU is a very popular activation function. Despite its simple behavior, ReLU still enables a neural network to learn **nonlinear** relationships between **features** and the **label**
#### Regression

*ML Fundamentals*

Informally, a model that generates a numerical prediction. (In contrast, a **classification model** generates a class prediction.) For example, the following are all regression models:

- A model that predicts a certain house's value, such as 426.000 Euros
- A model that predicts a certain tree's life expectancy, such as 23.3 years.
- A model that predicts the amount of rain that will fall in a certain city over the next six hours, such as 0.18 inches.

Two common types of regression models are:
- **Linear regression**, which finds the line that best fits label values to features
- **Logistic regression**, which generates a probability between 0.0 and 1.0 that a system typically then maps to a class prediction.

Not every model that outputs numerical predictions is a regression model. In some cases, a numeric prediction is really just a classification model that happens to have numeric class names. For example, a model that predicts a numeric postal code is a classification model, not a regression model.

#### Regularization

*ML Fundamentals*

Any mechanism that reduces **overfitting**. Popular types of regularization include:

- **L_1 regularization**
- **L_2 regularization**
- **dropout regularization**
- **early stopping** (this is not a formal regularization method, but can effectively limit overfitting)

Regularization can also be defined as the penalty on a model's complexity.

Regularization is counterintuitive. Increasing regularization usually increases training loss, which is confusing because, well, isn't the goal to minimize training loss?

Actually, no. The goal isn't to minimize training loss. The goal is to make excellent predictions on real-world examples. Remarkably, even though increasing regularization increases training loss, it usually helps models make better predictions on real-world examples.


#### Mean Squared Error (MSE)

The average loss per example when **L_2 loss** is used. Calculate Mean Squared Error as follows:
1. Calculate the L_2 loss for a batch
2. Divide the L_2 loss by the number of examples in the batch

$$Mean\ Squared\ Error = \frac{1}{n}\sum_{i=0}^n(y_i-\hat y_i)^2$$
where: 
- $n$ is the number of examples
- $y$ is the actual value of the label
- $\hat y$ is the model's prediction for $y$
#### Root Mean Squared Error (RMSE)

The square root of **Mean Squared**
$$Mean\ Squared\ Error = \sqrt{\frac{1}{n}\sum_{i=0}^n(y_i-\hat y_i)^2}$$

#### Sigmoid function

*ML Fundamentals*

A mathematical function that "squishes" an input value into a constrained range, typically 0 to 1 or -1 to +1. That is, you can pass any number (two, a million, negative billion, whatever) to a sigmoid and the output will still be in the constrained range. A plot of the sigmoid activation function looks as follows:
![[Sigmoid Function.png]]

The sigmoid function has several uses in machine learning, including:

- Converting the raw output of a **logistic regression** or **multinomial regression** model to a probability.
- Acting as an **activation function** in some neural networks.

The sigmoid function over an input number x has the following formula:

$$sigmoid(x) = \frac{1}{1+e^{-x}}$$
In machine learning, x is generally a **weighted sum**.
#### Single Linkage

Distance between nearest neighbors when computing clusters in **agglomerative hierarchical clustering**.

Encourages growth of elongated clusters.

#### Stochastic gradient descent

*ML Fundamentals*

A **gradient descent** algorithm in which the **batch size** is one. In other words, SGD trains on a single example chosen uniformly at random from a **training set**

#### Supervised learning

*ML Fundamentals*

Training a **model** from **features** and their corresponding **labels**. Supervised machine learning is analogous to learning a subject by studying a set of questions and their corresponding answers. After mastering the mapping between questions and answers, a student can then provide answers to new (never-before-seen) questions on the same topic.

Compare with **unsupervised machine learning**

#### Support Vector Machine (SVM)

A classification algorithm that seeks to maximize the margin between **positive** and **negative classes** by mapping input data vectors to a higher dimensional space. For example, consider a classification problem in which the input dataset has a hundred features. To maximize the margin between positive and negative classes, a SVM could internally map those features into a million-dimension space. SVMs uses a loss function called **hinge loss**.

#### Test data

A subset of the **dataset** reserved for testing a trained **model**.

Traditionally, you divide examples in the dataset into the following three distinct subsets:

- a **training set**
- a **validation set**
- a test set

Each example in a dataset should belong to only one of the preceding subsets. For instance, a single example should not belong to both the training set and the test set.

The training set and validation set are both closely tied to training a model. Because the test set is only indirectly associated with training, **test loss** is a less biased, higher quality metric than **training loss** or **validation loss**.

#### Training data

The subset of the **dataset** used to train a **model**.

#### Underfitting

*ML Fundamentals*

Producing a **model** with poor predictive ability because the model hasn't fully captured the complexity of the training data. Many problems can cause underfitting, including:

- Training on the wrong set of **features**
- Training for too few **epochs** or at too low a **learning rate**
- Training with too high a **regularization rate**
- Providing too few **hidden layers** in a deep neural network

#### Undersampling

Removing **examples** from the **majority class** in a **class-imbalanced dataset** in order to create a more balanced **training set**.

For example, consider a dataset in which the ratio of the majority class to the **minority class** is 20:1. To overcome this class imbalance, you could create a training set consisting of all of the minority class examples but only a tenth of the majority class examples, which would create a training-set class ratio of 2:1. Thanks to undersampling, this more balanced training set might produce a better model. Alternatively, this more balanced training set might contain insufficient examples to train an effective model.

Contrast with **oversampling**
#### Unsupervised learning

*Clustering, ML Fundamentals*

Training a **model** to find patterns in a dataset, typically an unlabeled dataset.

The most common use of unsupervised machine learning is to **cluster** data into groups of similar examples. For example, an unsupervised machine learning algorithm can cluster songs based on various properties of the music. The resulting clusters can become an input to other machine learning algorithms (for example, to a music recommendation service). Clustering can help when useful labels are scarce or absent. For example, in domains such as anti-abuse and fraud, clusters can help humans better understand the data.

Contrast with **supervised machine learning**

#### Validation data

*ML Fundamentals*

The subset of the **dataset** that performs initial evaluation against a trained **model**. Typically, you evaluate the trained model against the **validation set** several times before evaluating the model against the **test set**.

#### Variance

Error due to variations in the training set.
### Handwritten