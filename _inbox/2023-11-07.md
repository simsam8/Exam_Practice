### Notes

#### Complete linkage

*Clustering*
Complete linkage clustering (farthest neighbor) is one way to calculate distance between clusters in hierarchical clustering. The method is based on maximum distance; the similarity of any two clusters is the similarity of their most dissimilar pair.

Distance between the most distant elements in each cluster.

Behaves badly when **outliers** are present.
https://www.statisticshowto.com/complete-linkage-clustering/

#### Cross-validation
*ML Fundamentals*
A mechanism for estimating how well a **model** would generalize to new data by testing the model against one or more non-overlapping data subsets withheld from the **training set**



#### Deep learning

*ML Fundamentals*

Machine learning using a **neural network** containing more than one **hidden layer**.



#### Decision Boundary

The separator between **classes** learned by a **model** in a **binary class** or **multi-class classification problems**. For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange class and the blue class:

![[Pasted image 20231107123548.png]]

#### Decision Tree

*Decision Forests*

A supervised learning model composed of a set of **conditions** and **leaves** organized hierarchically. For example, the following is a decision tree:

![[Pasted image 20231107123712.png]]
#### Dimensionality reduction

Decreasing the number of dimensions used to represent a particular feature in a feature vector, typically by converting to an **embedding vector**


#### Ensemble methods

A collection of **models** trained independently whose predictions are averaged or aggregated. In many cases, an ensemble produces better predictions than a single model. For example, a **random forest** is an ensemble built from multiple **decision trees**. Not that not all **decision forests** are ensembles.

Types:
- **Bagging**, or Bootstrap Aggregating.
- Random Forest

#### Entropy

*Decision Forests*

In **information theory**, a description of how unpredictable a probability distribution is. Alternatively, entropy is also defined as how much information each **example** contains. A distribution has the highest possible entropy when all values of a random variable are equally likely.

The entropy of a set with two possible values "0" and "1" (for example, the labels in a **binary classification** problem) has the following formula:
$$H = -p\log{p} - q \log{q} = -p \log{p} - (1-p) * \log{(1-p)}$$

where:

- H is the entropy
- p is the fraction of "1" examples.
- q is the fraction of "0" examples. Note that q = (1 - p)
- log is generally $log_2$. In this case, the entropy unit is a bit.

For example, suppose the following:

- 100 examples contain the value "1"
- 300 examples contain the value "0"

Therefore, the entropy value is:

- p = 0.25
- q = 0.75
- $H = (-0.25)log_2 (0.25) - (0.75)log_2(0.75) = 0.81$ bits per example

A set that is perfectly balanced (for example, 200 "0"s and 200 "1"s) would have an entropy of 1.0 bit per example. As a set becomes more **imbalanced**, its entropy moves towards 0.

In **decision trees**, entropy helps formulate **information gain** to help the **splitter** select the **conditions** during the growth of a classification decision tree.

Compare entropy with:

- **gini impurity**
- **cross-entropy** loss function

Entropy is often called Shannon's entropy.

#### F1 score

A "roll-up" **binary classification** metric that relies on both **precision** and **recall**.
Here is the formula:

$$F_1 = \frac{2*precision*recall}{precision + recall}$$


#### Gradient descent

*ML Fundamentals*

A mathematical technique to minimize **loss**. Gradient descent iteratively adjusts **weights** and **biases**, gradually finding the best combination to minimize loss.

Gradient descent is older - much, much older - than machine learning.

#### Hierarchical clustering

*Clustering*

A category of **clustering** algorithms that create a tree of clusters. Hierarchical clustering is well-suited to hierarchical data, such as botanical taxonomies. There are two types of hierarchical clustering algorithm:

- **Agglomerative clustering** first assigns every example to its own cluster, and iteratively merges the closest clusters to create a hierarchical tree.
- **Divisive clustering** first groups all examples into one cluster and then iteratively divides the cluster into a hierarchical tree.

Contrast with **centroid-based clustering**
#### Imbalanced data

*ML Fundamentals*

A dataset for a classification problem in which the total number of **labels** of each class differs significantly. For example, consider a binary classification dataset whose two labels are divided as follows:

- 1,000,000 negative labels
- 10 positive labels

The ratio of negative to positive labels is 100,000 to 1, so this is a class-imbalanced dataset.

In contrast, the following dataset is not class-imbalanced because the ratio of negative labels to positive labels is relatively close to 1:

- 517 negative labels
- 483 positive labels

Multi-class datasets can also be class-imbalanced. For example, the following multi-class classification dataset is also class-imbalanced because one label has far more examples than the other two:

- 1,000,000 labels with class "green"
- 200 labels with class "purple"
- 350 labels with class "orange"
#### Imputation

Imputation/value imputation is the process of replacing a missing value with an acceptable substitute. When a value is missing, you can either discard the entire example or you can use value imputation to salvage the example.

For example, consider a dataset containing a temperature feature that is supposed to be recorded every hour. However, the temperature reading was unavailable for a particular hour. Here is a section of the dataset:

| Timestamp  | Temperature |
| ---------- | ----------- |
| 1680561000 | 10          |
| 1680564600 | 12          |
| 1680568200 | missing     |
| 1680571800 | 20          |
| 1680575400 | 21          |
| 1680579000 | 21          |

A system could either delete the missing example or impute the missing temperature as 12, 16, 18, or 20, depending on the imputation algorithm.


#### Information gain

*Decision Forests*

In **Decision forests**, the difference between a node's **entropy** and the weighted (by number of examples) sum of the entropy of its children nodes. A node's entropy is the entropy of the examples in that node.

For example, consider the following entropy values:

- entropy of parent node = 0.6
- entropy of one child node with 16 relevant examples = 0.2
- entropy of another child node with 24 relevant examples = 0.1

So 40% of the examples are in one child node and 60% are in the other child node. Therefore:
- weighted entropy sum of child nodes = (0.4 * 0.2) + (0.6 * 0.1) = 0.14

So, the information gain is:

- information gain = entropy of parent node - weighted entropy of sum of child nodes
- information gain = 0.6 = -0.14 = 0.46

Most **splitters** seek to create **conditions** that maximize information gain.


#### K-means

*Clustering*

A popular **clustering** algorithm that groups examples in unsupervised learning. The k-means algorithm basically does the following:

- Iteratively determines the best k center points (known as **centroids**).
- Assigns each example to the closest centroid. Those examples nearest the same centroid belong to the same group.

The k-means algorithm picks centroid locations to minimize the cumulative square of the distances from each example to its closest centroid.

For example, consider the following plot of dog heigh to dog width:

![[Pasted image 20231107144519.png]]

If k=3, the k-means algorithm will determine three centroids. Each example is assigned to its closest centroid, yielding three groups:
![[Pasted image 20231107144608.png]]

Imagine that a manufacturer wants to determine the ideal sizes for small, medium, and large sweaters for dogs. The three centroids identify the mean height and mean width of each dog in that cluster. So, the manufacturer should probably base sweater sizes on those three centroids. Note that the centroid of a cluster is typically not an example in the cluster.

The preceding illustrations shows k-means for examples with only two features (height and width). Note that k-mean can group examples across many features.



### Handwritten