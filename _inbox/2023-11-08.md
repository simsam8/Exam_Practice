### Notes


#### Curse of Dimensionality

*ML Fundamentals*

The Curse of Dimensionality refers to the various challenges and complications that arise when analyzing and organizing data in high-dimensional spaces (often hundreds or thousands of dimensions). In the realm of machine learning, it's crucial to understand this concept because as the number of features or dimensions in a dataset increases, the amount of data we need to generalize accurately grows exponentially.

Problems it causes:

- Data sparsity. Data becomes sparse, meaning that most of the high-dimensional space is empty. This makes clustering and classification tasks challenging.
- Increased computation. More dimensions mean more computational resources and time to process the data.
- Overfitting. With higher dimensions, models can become overly complex, fitting to the noise rather than the underlying pattern. This reduces the model's ability to generalize to new data.
- Distances lose meaning. In high dimensions, the difference in distances between data points tends to become negligible, making measures like **Euclidean distance** less meaningful.
- Performance degradation. Algorithms, especially those relaying on distance measurements like **k-nearest neighbors**, can see a drop in performance.
- Visualization challenges. High-dimensional data is hard to visualize, making exploratory data analysis more difficult.
#### Fairness
*AI ethics*

Machine learning models are often used to make decisions about people.
We would like those decisions to be fair.
Data are often produced by humans
- Humans are often biased (predjudices, stereotypes)
Machine learning models can learn the biases from the training data
#### Feature extraction

Create a (small) set of new variables that are informative (in some sense)
Feature extraction vs. feature engineering
- In feature engineering, one creates new features manually
- In feature extraction, new features are learned from the data

#### Feature selection

- Also known as variable selection, attribute or variable subset selection
- Which features can we ignore?
	- Irrelevant features
		- Constant features
		- Constant features with some noise
	- Redundant features
		- Features that is (linearly) dependent on other features
- Which features to keep?
	- Features that don't depend on others, i.e., uncorrelated -> low covariance
	- Features that change a lot (but not because of noise, high variance)
	- Features that are predictive

#### Generalization

*ML Fundamentals*

The ultimate goal of machine learning. Make predictions about examples that you have not seen.
#### Inductive bias

- Induction: from specific cases to general rules
- The inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered (Mitchell)
- A model without inductive bias can memorize training data but it cannot generalize
	- No basis for preferring one label over another on unseen data
- Note: your predictions depend crucially on your assumptions
#### Kernel

Transforms input data into a higher dimensional space  to make it linearly seperable.
#### K-nearest neighbors (k-NN)

- Inductive bias: similar objects have similar labels
- Possibly the simplest possible classifier:
	- Find k points that are most similar to the test point
	- Let them vote for the class label
- k is a parameter chosen by the modeller

- Large k reduces the effect of noise; small k increases flexibility
- For binary classification, an odd k avoids ties

#### Learning rate (step size)

*ML Fundamentals*

A floating-point number that tells the **gradient descent** algorithm how strongly to adjust weights and biases on each **iteration**. For example, a learning rate of 0.3 would adjust weights and biases three times more powerfully than a learning rate of 0.1.

Learning rate is a key **hyperparameter**. If you set the learning rate too low, training will take too long. If you set the learning rate too high, gradient descent often has trouble reaching **convergence**.

During each iteration, the **gradient descent** algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.
#### Linear Regression

*ML Fundamentals*

A type of machine learning model in which both of the following are true:
- The model is a **linear model**
- The prediction is a floating-point value. (This is the **regression** part of linear regression)

Contrast linear regression with **logistic regression**. Also, contrast regression with **classification**

#### Lloyd's algorithm

#### Logistic regression

*ML Fundamentals*

A type of **regression model** that predicts a probability. Logistic regression models have the following characteristics:

- The label is **categorical**. The term logistic regression usually refers to **binary logistic regression**, that is, to a model that calculates probabilities for labels with two possible values. A less common variant, multinomial logistic regression, calculates probabilities for labels with more than two possible values.

- The loss function during training is *Log Loss*. (Multiple Log Loss units can be placed in parallel for labels with more than two possible values.)
- The model has a linear architecture, not a deep neural network. However, the remainder of this definition also applies to **deep models** that predict probabilities for categorical labels.

For example, consider a logistic regression model that calculates the probability of an input email being either spam or not spam. During inference, suppose the model predicts 0.72. Therefore, the model is estimating:

- A 72% chance of the email being spam.
- A 28% chance of the email not being spam.

A logistic regression model uses the following two-step architecture:

1. The model generates a raw prediction (y') by applying a linear function of input features
2. The model uses that raw prediction to a value between 0 and 1, exclusive.

Like any regression model, a logistic regression model predicts a number.
However, this number typically becomes part of a binary classification model as follows:

- If the predicted number is greater than the **classification threshold**, the binary classification model predicts the positive class.
- If the predicted number is less than the classification threshold, the binary classification model predicts the negative class.
#### Loss function (cost function)

*ML Fundamentals*

During **training** or testing, a mathematical function that calculates the loss on a **batch** of examples. A loss function returns a lower loss for models that makes good predictions than for models that make bad predictions.

The goal of training is typically to minimize the loss that a loss function returns.

Many different kinds of loss functions exist. Pick the appropriate loss function for the kind of model you are building. For example:

- **$L_2$ loss** (or **Mean Squared Error**) is the loss function for **linear regression**
- **Log Loss** is the loss function for **logistic regression**

#### Mean Absolute Error (MAE)

The average loss per example when **$L_1$ loss** is used. Calculate Mean Absolute Error as follows:

1. Calculate the $L_1$ loss for a batch.
2. Divide the $L_1$ loss by the number of examples in the batch.

$$Mean\ Absolute \ Error = \frac{1}{n}\sum_{i=0}^{n}{|y_i - \hat y_i|}$$
where:
- $n$ is the number of examples.
- $y$ is the actual value of the label.
- $\hat y$ is the value that the model predicts for $y$

#### Missing data (MCAR, MAR, MNAR)

- "Holes" in the data matrix that we ideally would have observed.
	- Missing items have a meaningful (but unobserved) value
- Reasons for missing data:
	- Non-responses in questionnaires
	- Manual data entry
	- Measurement errors
	- $\dots$

- Missing completely at random (MCAR)
	- The probability of missingness does not depend on the value of the missing variable or any other variable
- Missing at random (MAR)
	- The probability of missingness depends on values of other features but not on the value of the missing feature itself
- Not Missing at random (NMAR) or Missing Not at Random (MNAR)
	- The probability of missingness depends on the value of the variable itself
### Handwritten